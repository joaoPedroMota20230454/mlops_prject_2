most pressing:
> in model selection:
    - finish nodes
    - finish pipelines
    - add to pipeline registry


> better cleaning
> better expectation suites
> more feature engineering
> data drift
> unit tests with pytest
> streamlit (or any additional bonus tech.)
> why is git not ignoring the credentials.yml even though it's in the ignore file?
> we are dropping so many rows during cleaning that we will need to have a way to handle them during model predictions, e.g. if we drop rows where diag. is '?' what do we do when when we see them in new data? For simplicity's sake we are better off by replacing them by nulls and then impute the nulls.
> logging
> in feature selection if using RFE add the possibility of using the curr. champion model as the model to test.
> feature selection depends on the algorithm, whenever we choose an algo. and the function outputs a list with the chosen variables we will overwite the data in the 06_pruned folder. To track this we should use mlflow capabilities.

Pipelines:
1. experimental pipeline
2. production pipeline
3. data drift pipeline


Bonus ideas:
> trigger model retraining pipeline
> pipeline documentation flow (i.e. with kedro viz)
> to simulate a real scenario our raw data instead can come from a sqlite db instead of a csv file. (would need to add data ingestion pipeline + structure db + query db, should be easy)

> train-val-test split to avoid complexity. if only train-test we would need to tune our models using CV and, to avoid data leakage,
 we would need to tune entire pipelines and not just the classifier. E.g. say we had numeric and categorical variables: we would
  need to 1st tune a column transformer to impute numeric and categorical separetely. For the numeric we might add a scaler,
   and then at the end have the classifier.... would be quite bothersome.

> when running the model selection pipeline (kedro run --pipeline model_selection) we were getting a weird error 
along the lines of "there is already an active run". Which was weird because when testing model_selection 
(the nodes.py file, not the pipeline) everything was running smoothly. 
Turns out that when we start running a pipeline MLFlow tracks that pipeline run as a MLFlow run
 (under an experiment with the same name as the project), meaning that when running the actual function for
  model selection in the nodes.py file there would be an issue as we would have 2 independent runs running at the same time.
   The solution was to set the "parent" run the select_model function to "nested". Now we have 3 levels 
   of nesting: pipeline run -> model selection run -> indep. model in model selection run.
> we are sending our train and test data to the feature store separately, should we do this?

> how to deal with overfitting inside the pipeline? currently (if you go into model selection) we are checking overfitting by comparing the f1 scores in the train+val and test sets. if the absolute difference is larger than .1 we consider it too much overfitting and 'discard' the model, i.e. cannot be best model

> should we only have the champion model registered?

> we've seen that whenever we create a new pipeline using the kedro CLI a new parameter file is also created, e.g. parameters_model_selection.yml, how to call these files in the pipelines?

> how would the pipelines change if we were using models not just from sklearn? ans: by wrapping models in base estimators and classifier mixin:
e.g.
from sklearn.base import BaseEstimator, ClassifierMixin
import xgboost as xgb
import numpy as np

class XGBoostWrapper(BaseEstimator, ClassifierMixin):
    def __init__(self, **kwargs):
        self.params = kwargs
        self.model = None

    def fit(self, X, y):
        dtrain = xgb.DMatrix(X, label=y)
        self.model = xgb.train(self.params, dtrain)
        return self

    def predict(self, X):
        dtest = xgb.DMatrix(X)
        return np.round(self.model.predict(dtest))

    def predict_proba(self, X):
        dtest = xgb.DMatrix(X)
        return self.model.predict(dtest)

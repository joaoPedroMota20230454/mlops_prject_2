> we are sending our train and test data to the feature store separately, should we do this?

> how to deal with overfitting inside the pipeline? currently (if you go into model selection) we are checking overfitting 
by comparing the f1 scores in the train+val and test sets. if the absolute difference is larger than .1 we consider it too much 
overfitting and 'discard' the model, i.e. cannot be best model

> should we only have the champion model registered?

> is prod fully independent from dev?

> in prod. do we have access to the true labels? i.e. if yes we could also evaluate our model, but if not then the prod pipelines will end in a prediction step?

# DONT FORGET TO SAVE SKLEARN SCALERS / IMPUTERS / ETC AND LOAD THEM DURING INFERENCE

> should we save the predictions history? if yes, how? should we track it with mlflow

> if we have no labels in prod then how to check for label drift?

> where should we keep the "new data" the data not using for training but for predicting and checking for drift?

> is data drift its own pipeline or should it be connected to dev and/or prod pipelines?

> where in the pipelines to add the explainability stuff? should we add a pipeline after champion selection?

> how would the pipelines change if we were using models not just from sklearn? ans: by wrapping models in base estimators 
and classifier mixin:
e.g.
from sklearn.base import BaseEstimator, ClassifierMixin
import xgboost as xgb
import numpy as np

class XGBoostWrapper(BaseEstimator, ClassifierMixin):
    def __init__(self, **kwargs):
        self.params = kwargs
        self.model = None

    def fit(self, X, y):
        dtrain = xgb.DMatrix(X, label=y)
        self.model = xgb.train(self.params, dtrain)
        return self

    def predict(self, X):
        dtest = xgb.DMatrix(X)
        return np.round(self.model.predict(dtest))

    def predict_proba(self, X):
        dtest = xgb.DMatrix(X)
        return self.model.predict(dtest)
